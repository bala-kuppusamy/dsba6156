{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Execute Classification Analysis for Type"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier as DTree\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_classification_for_model(_model: DTree, _x_train, _x_test, _y_train, _y_test) -> None:\n",
    "    _model.fit(_x_train, _y_train)\n",
    "    y_pred = _model.predict(_x_test)\n",
    "\n",
    "    #compare y_pred with actual targets for your test set(y_test) and calculate precision, recall, f1-score\n",
    "    print(\"Precision: %0.2f\" %precision_score(_y_test, y_pred , average=\"macro\"))\n",
    "    print(\"Recall:  %0.2f\" %recall_score(_y_test, y_pred , average=\"macro\"))\n",
    "    print(\"F1-score:  %0.2f\" %f1_score(_y_test, y_pred , average=\"macro\"))\n",
    "\n",
    "    print(confusion_matrix(_y_test, y_pred))\n",
    "    print(classification_report(_y_test, y_pred))\n",
    "\n",
    "def run_cross_validation_for_model(_model, _x, _y) -> None:\n",
    "    x, y = shuffle(_x, _y)\n",
    "    precision = cross_val_score(_model, x, y, cv=10, scoring='precision')\n",
    "    recall = cross_val_score(_model, x, y, cv=10, scoring='recall')\n",
    "    f1 = cross_val_score(_model, x, y, cv=10, scoring='f1')\n",
    "\n",
    "    print(\"Precision: %0.2f (+/- %0.2f)\" % (precision.mean(), precision.std() * 2))\n",
    "    print(\"Recall: %0.2f (+/- %0.2f)\" % (recall.mean(), recall.std() * 2))\n",
    "    print(\"F1-score: %0.2f (+/- %0.2f)\" % (f1.mean(), f1.std() * 2))\n",
    "\n",
    "def run_model_multinomial(x_train, x_test, y_train, y_test) -> None:\n",
    "    print(\"Multinomial Naive-Bayes Classifier\")\n",
    "    bayes = MultinomialNB()\n",
    "    run_classification_for_model(bayes, x_train, x_test, y_train, y_test)\n",
    "    # run_cross_validation_for_model(bayes, _x, _y)\n",
    "\n",
    "def run_model_gaussian(x_train, x_test, y_train, y_test) -> None:\n",
    "    print(\"Gaussian Naive-Bayes Classifier\")\n",
    "    bayes = GaussianNB()\n",
    "    run_classification_for_model(bayes, x_train, x_test, y_train, y_test)\n",
    "    # run_cross_validation_for_model(bayes, _x, _y)\n",
    "\n",
    "def run_model_bernoulli(x_train, x_test, y_train, y_test) -> None:\n",
    "    print(\"Bernoulli Naive-Bayes Classifier\")\n",
    "    bayes = BernoulliNB()\n",
    "    run_classification_for_model(bayes, x_train, x_test, y_train, y_test)\n",
    "    # run_cross_validation_for_model(bayes, _x, _y)\n",
    "\n",
    "def run_model_decision_tree(x_train, x_test, y_train, y_test) -> None:\n",
    "    print(\"Decision Tree Classifier\")\n",
    "    d_tree = DTree(criterion='entropy')\n",
    "    run_classification_for_model(d_tree, x_train, x_test, y_train, y_test)\n",
    "    d_tree_clf = tree.DecisionTreeClassifier()\n",
    "    # run_cross_validation_for_model(d_tree_clf, _x, _y)\n",
    "\n",
    "def run_all_classifications(_x: DataFrame, _y: DataFrame) -> None:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.2, random_state=1)\n",
    "\n",
    "    # run_model_multinomial(x_train, x_test, y_train, y_test)\n",
    "    run_model_gaussian(x_train, x_test, y_train, y_test)\n",
    "    run_model_bernoulli(x_train, x_test, y_train, y_test)\n",
    "    run_model_decision_tree(x_train, x_test, y_train, y_test)\n",
    "\n",
    "def get_final_model(_x: DataFrame, _y: DataFrame):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.2, random_state=1)\n",
    "    d_tree = DTree(criterion='entropy')\n",
    "    _model = d_tree.fit(x_train, y_train)\n",
    "    return _model\n",
    "\n",
    "def run_final_model(_model, _x_test: DataFrame, _y_test: DataFrame) -> None:\n",
    "    y_pred = _model.predict(_x_test)\n",
    "\n",
    "    print(\"Precision: %0.2f\" %precision_score(_y_test, y_pred , average=\"macro\"))\n",
    "    print(\"Recall:  %0.2f\" %recall_score(_y_test, y_pred , average=\"macro\"))\n",
    "    print(\"F1-score:  %0.2f\" %f1_score(_y_test, y_pred , average=\"macro\"))\n",
    "\n",
    "    print(confusion_matrix(_y_test, y_pred))\n",
    "    print(classification_report(_y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read training dataset from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(344667, 119)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "master: DataFrame = pd.read_pickle('./data/master.pickle')\n",
    "print(master.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select features for the classification analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features_0 = ['Size']\n",
    "features_1 = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size']\n",
    "features_2 = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size', 'Weekly_Sales', 'Dept', 'IsHoliday', 'HasMarkDown']\n",
    "\n",
    "target = ['Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Gaussian Naive-Bayes Classifier\n",
      "Precision: 0.87\n",
      "Recall:  0.94\n",
      "F1-score:  0.89\n[[33201     0  2086]\n [ 1603 23644  1537]\n [    0     0  6863]]\n",
      "              precision    recall  f1-score   support\n\n           A       0.95      0.94      0.95     35287\n           B       1.00      0.88      0.94     26784\n           C       0.65      1.00      0.79      6863\n\n    accuracy                           0.92     68934\n   macro avg       0.87      0.94      0.89     68934\nweighted avg       0.94      0.92      0.93     68934\n\nBernoulli Naive-Bayes Classifier\n",
      "Precision: 0.17\n",
      "Recall:  0.33\n",
      "F1-score:  0.23\n",
      "[[35287     0     0]\n [26784     0     0]\n [ 6863     0     0]]\n",
      "              precision    recall  f1-score   support\n\n           A       0.51      1.00      0.68     35287\n           B       0.00      0.00      0.00     26784\n           C       0.00      0.00      0.00      6863\n\n    accuracy                           0.51     68934\n   macro avg       0.17      0.33      0.23     68934\nweighted avg       0.26      0.51      0.35     68934\n\nDecision Tree Classifier\n",
      "Precision: 0.92\n",
      "Recall:  0.98\n",
      "F1-score:  0.95\n",
      "[[33201     0  2086]\n [    0 26784     0]\n [    0     0  6863]]\n",
      "              precision    recall  f1-score   support\n\n           A       1.00      0.94      0.97     35287\n           B       1.00      1.00      1.00     26784\n           C       0.77      1.00      0.87      6863\n\n    accuracy                           0.97     68934\n   macro avg       0.92      0.98      0.95     68934\nweighted avg       0.98      0.97      0.97     68934\n\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "run_all_classifications(master[features_0], master[target])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Gaussian Naive-Bayes Classifier\n",
      "Precision: 0.87\n",
      "Recall:  0.94\n",
      "F1-score:  0.89\n",
      "[[33201     0  2086]\n [ 1603 23644  1537]\n [    0     0  6863]]\n",
      "              precision    recall  f1-score   support\n\n           A       0.95      0.94      0.95     35287\n           B       1.00      0.88      0.94     26784\n           C       0.65      1.00      0.79      6863\n\n    accuracy                           0.92     68934\n   macro avg       0.87      0.94      0.89     68934\nweighted avg       0.94      0.92      0.93     68934\n\nBernoulli Naive-Bayes Classifier\n",
      "Precision: 0.50\n",
      "Recall:  0.33\n",
      "F1-score:  0.23\n",
      "[[35287     0     0]\n [26776     8     0]\n [ 6863     0     0]]\n",
      "              precision    recall  f1-score   support\n\n           A       0.51      1.00      0.68     35287\n           B       1.00      0.00      0.00     26784\n           C       0.00      0.00      0.00      6863\n\n    accuracy                           0.51     68934\n   macro avg       0.50      0.33      0.23     68934\nweighted avg       0.65      0.51      0.35     68934\n\nDecision Tree Classifier\n",
      "Precision: 1.00\n",
      "Recall:  1.00\n",
      "F1-score:  1.00\n",
      "[[35276     0    11]\n [    0 26784     0]\n [    0     0  6863]]\n",
      "              precision    recall  f1-score   support\n\n           A       1.00      1.00      1.00     35287\n           B       1.00      1.00      1.00     26784\n           C       1.00      1.00      1.00      6863\n\n    accuracy                           1.00     68934\n   macro avg       1.00      1.00      1.00     68934\nweighted avg       1.00      1.00      1.00     68934\n\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "run_all_classifications(master[features_1], master[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Gaussian Naive-Bayes Classifier\n",
      "Precision: 0.86\n",
      "Recall:  0.93\n",
      "F1-score:  0.88\n",
      "[[33101   100  2086]\n [ 2516 22758  1510]\n [   20     7  6836]]\n",
      "              precision    recall  f1-score   support\n\n           A       0.93      0.94      0.93     35287\n           B       1.00      0.85      0.92     26784\n           C       0.66      1.00      0.79      6863\n\n    accuracy                           0.91     68934\n   macro avg       0.86      0.93      0.88     68934\nweighted avg       0.93      0.91      0.91     68934\n\nBernoulli Naive-Bayes Classifier\n",
      "Precision: 0.36\n",
      "Recall:  0.33\n",
      "F1-score:  0.23\n",
      "[[35211    76     0]\n [26664   120     0]\n [ 6843    20     0]]\n",
      "              precision    recall  f1-score   support\n\n           A       0.51      1.00      0.68     35287\n           B       0.56      0.00      0.01     26784\n           C       0.00      0.00      0.00      6863\n\n    accuracy                           0.51     68934\n   macro avg       0.36      0.33      0.23     68934\nweighted avg       0.48      0.51      0.35     68934\n\nDecision Tree Classifier\n",
      "Precision: 1.00\n",
      "Recall:  1.00\n",
      "F1-score:  1.00\n",
      "[[35275     0    12]\n [    0 26784     0]\n [    4     0  6859]]\n",
      "              precision    recall  f1-score   support\n\n           A       1.00      1.00      1.00     35287\n           B       1.00      1.00      1.00     26784\n           C       1.00      1.00      1.00      6863\n\n    accuracy                           1.00     68934\n   macro avg       1.00      1.00      1.00     68934\nweighted avg       1.00      1.00      1.00     68934\n\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "run_all_classifications(master[features_2], master[target])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Running finalized model on test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(76903, 113)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "test: DataFrame = pd.read_pickle('./data/test.pickle')\n",
    "print(test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "model = get_final_model(master[features_1], master[target])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Precision: 0.94\n",
      "Recall:  0.98\n",
      "F1-score:  0.95\n",
      "[[37402     0  1797]\n [    0 29743     0]\n [  218     0  7743]]\n",
      "              precision    recall  f1-score   support\n\n           A       0.99      0.95      0.97     39199\n           B       1.00      1.00      1.00     29743\n           C       0.81      0.97      0.88      7961\n\n    accuracy                           0.97     76903\n   macro avg       0.94      0.98      0.95     76903\nweighted avg       0.98      0.97      0.97     76903\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "run_final_model(model, test[features_1], test[target])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (dsba6156)",
   "language": "python",
   "name": "pycharm-67ea9c55"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "#make the necessary imports and load the data and answer the following Questions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier as DTree\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score,classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Decision Trees and Multinomial Naive Bayes\n",
    "This question requires you to classify wine based on the quality ('quality' is your target variable) using the features available to you in winequality-white.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.0              0.27         0.36            20.7      0.045   \n",
      "1            6.3              0.30         0.34             1.6      0.049   \n",
      "2            8.1              0.28         0.40             6.9      0.050   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
      "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
      "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      8.8        6  \n",
      "1      9.5        6  \n",
      "2     10.1        6  \n"
     ]
    }
   ],
   "source": [
    "#Load 'winequality-white.csv'; use separator as ';'\n",
    "\n",
    "wine = pd.read_csv('winequality-white.csv', delimiter=';')\n",
    "\n",
    "print(wine.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration & Wrangling (5 questions * 4 points)\n",
    "1.\tWhat is the structure of your dataset? (shape)\n",
    "2.\tWhat is/are the main feature(s) in your dataset? (column names)\n",
    "3.\tList the features as Categorical or Continuous. (head/tail)\n",
    "4.\tDescribe the statistical features (viz. mean, median, standard deviation) of these features?\n",
    "6.\tAre there missing values in your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4898, 12)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 shape\n",
    "wine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
       "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
       "       'pH', 'sulphates', 'alcohol', 'quality'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 features\n",
    "wine.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      "fixed acidity           4898 non-null float64\n",
      "volatile acidity        4898 non-null float64\n",
      "citric acid             4898 non-null float64\n",
      "residual sugar          4898 non-null float64\n",
      "chlorides               4898 non-null float64\n",
      "free sulfur dioxide     4898 non-null float64\n",
      "total sulfur dioxide    4898 non-null float64\n",
      "density                 4898 non-null float64\n",
      "pH                      4898 non-null float64\n",
      "sulphates               4898 non-null float64\n",
      "alcohol                 4898 non-null float64\n",
      "quality                 4898 non-null int64\n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.2 KB\n"
     ]
    }
   ],
   "source": [
    "#3 categorical vs continuous\n",
    "wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "      <td>4898.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>6.854788</td>\n",
       "      <td>0.278241</td>\n",
       "      <td>0.334192</td>\n",
       "      <td>6.391415</td>\n",
       "      <td>0.045772</td>\n",
       "      <td>35.308085</td>\n",
       "      <td>138.360657</td>\n",
       "      <td>0.994027</td>\n",
       "      <td>3.188267</td>\n",
       "      <td>0.489847</td>\n",
       "      <td>10.514267</td>\n",
       "      <td>5.877909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.843868</td>\n",
       "      <td>0.100795</td>\n",
       "      <td>0.121020</td>\n",
       "      <td>5.072058</td>\n",
       "      <td>0.021848</td>\n",
       "      <td>17.007137</td>\n",
       "      <td>42.498065</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.151001</td>\n",
       "      <td>0.114126</td>\n",
       "      <td>1.230621</td>\n",
       "      <td>0.885639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.991723</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.993740</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>0.996100</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.038980</td>\n",
       "      <td>3.820000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    4898.000000       4898.000000  4898.000000     4898.000000   \n",
       "mean        6.854788          0.278241     0.334192        6.391415   \n",
       "std         0.843868          0.100795     0.121020        5.072058   \n",
       "min         3.800000          0.080000     0.000000        0.600000   \n",
       "25%         6.300000          0.210000     0.270000        1.700000   \n",
       "50%         6.800000          0.260000     0.320000        5.200000   \n",
       "75%         7.300000          0.320000     0.390000        9.900000   \n",
       "max        14.200000          1.100000     1.660000       65.800000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  4898.000000          4898.000000           4898.000000  4898.000000   \n",
       "mean      0.045772            35.308085            138.360657     0.994027   \n",
       "std       0.021848            17.007137             42.498065     0.002991   \n",
       "min       0.009000             2.000000              9.000000     0.987110   \n",
       "25%       0.036000            23.000000            108.000000     0.991723   \n",
       "50%       0.043000            34.000000            134.000000     0.993740   \n",
       "75%       0.050000            46.000000            167.000000     0.996100   \n",
       "max       0.346000           289.000000            440.000000     1.038980   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  4898.000000  4898.000000  4898.000000  4898.000000  \n",
       "mean      3.188267     0.489847    10.514267     5.877909  \n",
       "std       0.151001     0.114126     1.230621     0.885639  \n",
       "min       2.720000     0.220000     8.000000     3.000000  \n",
       "25%       3.090000     0.410000     9.500000     5.000000  \n",
       "50%       3.180000     0.470000    10.400000     6.000000  \n",
       "75%       3.280000     0.550000    11.400000     6.000000  \n",
       "max       3.820000     1.080000    14.200000     9.000000  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4 statistical information for the continuous variables\n",
    "wine.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           0\n",
       "volatile acidity        0\n",
       "citric acid             0\n",
       "residual sugar          0\n",
       "chlorides               0\n",
       "free sulfur dioxide     0\n",
       "total sulfur dioxide    0\n",
       "density                 0\n",
       "pH                      0\n",
       "sulphates               0\n",
       "alcohol                 0\n",
       "quality                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5 missing values\n",
    "wine.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection and Conditioning\n",
    "1. Instead of considering all the features at once, take the following 3 subsets of these features:\n",
    "\n",
    "A = ['fixed acidity', 'free sulfur dioxide', 'citric acid', 'residual sugar', 'alcohol']\n",
    "\n",
    "B = ['chlorides', 'sulphates', 'total sulfur dioxide', 'density']\n",
    "\n",
    "C = ['citric acid', 'volatile acidity', 'alcohol','density','chlorides']\n",
    "\n",
    "For each feature set, train a classifier and report the Scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Treatment A\n",
    "\n",
    "- get the feature set for Treatment A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the feature set A: (4898, 5)\n"
     ]
    }
   ],
   "source": [
    "target = wine[['quality']]\n",
    "\n",
    "feat_a = wine[['fixed acidity', 'free sulfur dioxide', 'citric acid', 'residual sugar', 'alcohol']]\n",
    "print('Shape of the feature set A:', feat_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (2 questions * 2.5 points)\n",
    "\n",
    "1.\tReport the dimensions and type after separating the dataset to predictors(X) and target(y)\n",
    "\n",
    "2.\tPerform train-test split. Set test_size to 0.1 and random_state to 42 and report the dimensions of X_test and y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (4898, 5)\n",
      "Shape of y: (4898, 1)\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "X = feat_a\n",
    "y = target\n",
    "\n",
    "print('Shape of X:', X.shape)\n",
    "print('Shape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test: (490, 5)\n",
      "Shape of y_test: (490, 1)\n"
     ]
    }
   ],
   "source": [
    "#2 train/test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print('Shape of X_test:', X_test.shape)\n",
    "print('Shape of y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling (2 questions * 2.5 points)\n",
    "1. Devise 2 models for our problem. \n",
    "\n",
    "a.\tDecision Tree Classifier\n",
    "\n",
    "b.\tMultinomial NB Classifier **(Hint: This is designed to handle multiclass target values; has the same function signature as Bernoulli NB; use ‘from sklearn.naive_bayes import MultinomialNB’)**\n",
    "\n",
    "(Both of these classifiers are inherently multiclass i.e. can handle >=2 and you don't need any extra parameters)\n",
    "\n",
    "2. Give the f1_score, precision_score, recall_score for both the classifiers. Also print the classification report. Which of the 2 classifiers performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_pred: (490,)\n"
     ]
    }
   ],
   "source": [
    "#1 Decision Tree classifier\n",
    "model_dt = DTree(criterion='entropy')\n",
    "model_dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = model_dt.predict(X_test)\n",
    "print('Shape of y_pred:', y_pred_dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.43\n",
      "Recall:  0.43\n",
      "F1-score:  0.43\n",
      "[[  0   0   2   1   0   0]\n",
      " [  0   6   8   4   0   0]\n",
      " [  0   7  86  40   9   2]\n",
      " [  0   3  41 140  28   3]\n",
      " [  0   3   9  23  56   3]\n",
      " [  0   0   0   4   6   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.32      0.33      0.32        18\n",
      "           5       0.59      0.60      0.59       144\n",
      "           6       0.66      0.65      0.66       215\n",
      "           7       0.57      0.60      0.58        94\n",
      "           8       0.43      0.38      0.40        16\n",
      "\n",
      "    accuracy                           0.60       490\n",
      "   macro avg       0.43      0.43      0.43       490\n",
      "weighted avg       0.60      0.60      0.60       490\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#2 DT Scores\n",
    "\n",
    "print(\"Precision: %0.2f\" %precision_score(y_test, y_pred_dt , average=\"macro\"))\n",
    "print(\"Recall:  %0.2f\" %recall_score(y_test, y_pred_dt , average=\"macro\"))\n",
    "print(\"F1-score:  %0.2f\" %f1_score(y_test, y_pred_dt , average=\"macro\"))\n",
    "print(confusion_matrix(y_test, y_pred_dt))\n",
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_pred: (490,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#1 Multinomial NB classifier\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_nb = model_nb.predict(X_test)\n",
    "print('Shape of y_pred:', y_pred_nb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.23\n",
      "Recall:  0.23\n",
      "F1-score:  0.22\n",
      "[[  0   0   2   1   0   0]\n",
      " [  0   6   3   8   1   0]\n",
      " [  5  13  41  82   3   0]\n",
      " [  2   8  44 142  19   0]\n",
      " [  2   4  11  65  12   0]\n",
      " [  0   0   4  12   0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.19      0.33      0.24        18\n",
      "           5       0.39      0.28      0.33       144\n",
      "           6       0.46      0.66      0.54       215\n",
      "           7       0.34      0.13      0.19        94\n",
      "           8       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.41       490\n",
      "   macro avg       0.23      0.23      0.22       490\n",
      "weighted avg       0.39      0.41      0.38       490\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#2 Multinomial NB Scores\n",
    "print(\"Precision: %0.2f\" %precision_score(y_test, y_pred_nb , average=\"macro\"))\n",
    "print(\"Recall:  %0.2f\" %recall_score(y_test, y_pred_nb , average=\"macro\"))\n",
    "print(\"F1-score:  %0.2f\" %f1_score(y_test, y_pred_nb , average=\"macro\"))\n",
    "print(confusion_matrix(y_test, y_pred_nb))\n",
    "print(classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of the 2 models\n",
    "\n",
    "- The Precision, Recall & F1 scores for DecisionTree model is 0.41, whereas the same for Multinomial NB model is around 0.23.\n",
    "- Based on this analysis, the DecisionTree model performs better for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Treatment B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (2 questions * 2.5 points)\n",
    "\n",
    "1.\tReport the dimensions and type after separating the dataset to predictors(X) and target(y)\n",
    "\n",
    "2.\tPerform train-test split. Set test_size to 0.1 and random_state to 42 and report the dimensions of X_test and y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the feature set B: (4898, 4)\n",
      "Shape of X: (4898, 4)\n",
      "Shape of y: (4898, 1)\n"
     ]
    }
   ],
   "source": [
    "feat_b = wine[['chlorides', 'sulphates', 'total sulfur dioxide', 'density']]\n",
    "print('Shape of the feature set B:', feat_b.shape)\n",
    "\n",
    "#1\n",
    "X = feat_b\n",
    "y = target\n",
    "\n",
    "print('Shape of X:', X.shape)\n",
    "print('Shape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test: (490, 4)\n",
      "Shape of y_test: (490, 1)\n"
     ]
    }
   ],
   "source": [
    "#2 train/test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print('Shape of X_test:', X_test.shape)\n",
    "print('Shape of y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling (2 questions * 2.5 points)\n",
    "1. Devise 2 models for our problem. \n",
    "\n",
    "a.\tDecision Tree Classifier\n",
    "\n",
    "b.\tMultinomial NB Classifier **(Hint: This is designed to handle multiclass target values; has the same function signature as Bernoulli NB; use ‘from sklearn.naive_bayes import MultinomialNB’)**\n",
    "\n",
    "(Both of these classifiers are inherently multiclass i.e. can handle >=2 and you don't need any extra parameters)\n",
    "\n",
    "2. Give the f1_score, precision_score, recall_score for both the classifiers. Also print the classification report. Which of the 2 classifiers performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_pred: (490,)\n"
     ]
    }
   ],
   "source": [
    "#1 Decision Tree classifier\n",
    "model_dt = DTree(criterion='entropy')\n",
    "model_dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = model_dt.predict(X_test)\n",
    "print('Shape of y_pred:', y_pred_dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.40\n",
      "Recall:  0.43\n",
      "F1-score:  0.41\n",
      "[[  0   0   2   1   0   0]\n",
      " [  0   3   5   8   1   1]\n",
      " [  1   4  93  33   9   4]\n",
      " [  0   4  32 143  27   9]\n",
      " [  0   2   9  29  50   4]\n",
      " [  0   0   0   2   5   9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.23      0.17      0.19        18\n",
      "           5       0.66      0.65      0.65       144\n",
      "           6       0.66      0.67      0.66       215\n",
      "           7       0.54      0.53      0.54        94\n",
      "           8       0.33      0.56      0.42        16\n",
      "\n",
      "    accuracy                           0.61       490\n",
      "   macro avg       0.40      0.43      0.41       490\n",
      "weighted avg       0.61      0.61      0.61       490\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2 DT Scores\n",
    "print(\"Precision: %0.2f\" %precision_score(y_test, y_pred_dt , average=\"macro\"))\n",
    "print(\"Recall:  %0.2f\" %recall_score(y_test, y_pred_dt , average=\"macro\"))\n",
    "print(\"F1-score:  %0.2f\" %f1_score(y_test, y_pred_dt , average=\"macro\"))\n",
    "print(confusion_matrix(y_test, y_pred_dt))\n",
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_pred: (490,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#1 Multinomial NB classifier\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_nb = model_nb.predict(X_test)\n",
    "print('Shape of y_pred:', y_pred_nb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.07\n",
      "Recall:  0.17\n",
      "F1-score:  0.10\n",
      "[[  0   0   0   3   0   0]\n",
      " [  0   0   0  18   0   0]\n",
      " [  0   0   0 144   0   0]\n",
      " [  0   0   0 215   0   0]\n",
      " [  0   0   0  94   0   0]\n",
      " [  0   0   0  16   0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00       144\n",
      "           6       0.44      1.00      0.61       215\n",
      "           7       0.00      0.00      0.00        94\n",
      "           8       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.44       490\n",
      "   macro avg       0.07      0.17      0.10       490\n",
      "weighted avg       0.19      0.44      0.27       490\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#2 Multinomial NB Scores\n",
    "print(\"Precision: %0.2f\" %precision_score(y_test, y_pred_nb , average=\"macro\"))\n",
    "print(\"Recall:  %0.2f\" %recall_score(y_test, y_pred_nb , average=\"macro\"))\n",
    "print(\"F1-score:  %0.2f\" %f1_score(y_test, y_pred_nb , average=\"macro\"))\n",
    "print(confusion_matrix(y_test, y_pred_nb))\n",
    "print(classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of the 2 models\n",
    "\n",
    "- The Precision, Recall & F1 scores for DecisionTree model is around 0.43, whereas the same for Multinomial NB model is between 0.7 - 0.17.\n",
    "- Based on this analysis, the DecisionTree model performs better for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Treatment C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (2 questions * 2.5 points)\n",
    "\n",
    "1.\tReport the dimensions and type after separating the dataset to predictors(X) and target(y)\n",
    "\n",
    "2.\tPerform train-test split. Set test_size to 0.1 and random_state to 42 and report the dimensions of X_test and y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the feature set C: (4898, 5)\n",
      "Shape of X: (4898, 5)\n",
      "Shape of y: (4898, 1)\n"
     ]
    }
   ],
   "source": [
    "feat_c = wine[['citric acid', 'volatile acidity', 'alcohol','density','chlorides']]\n",
    "print('Shape of the feature set C:', feat_c.shape)\n",
    "\n",
    "#1\n",
    "X = feat_c\n",
    "y = target\n",
    "\n",
    "print('Shape of X:', X.shape)\n",
    "print('Shape of y:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test: (490, 5)\n",
      "Shape of y_test: (490, 1)\n"
     ]
    }
   ],
   "source": [
    "#2 train/test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print('Shape of X_test:', X_test.shape)\n",
    "print('Shape of y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling (2 questions * 2.5 points)\n",
    "\n",
    "1. Devise 2 models for our problem. \n",
    "\n",
    "a.\tDecision Tree Classifier\n",
    "\n",
    "b.\tMultinomial NB Classifier **(Hint: This is designed to handle multiclass target values; has the same function signature as Bernoulli NB; use ‘from sklearn.naive_bayes import MultinomialNB’)**\n",
    "\n",
    "(Both of these classifiers are inherently multiclass i.e. can handle >=2 and you don't need any extra parameters)\n",
    "\n",
    "2. Give the f1_score, precision_score, recall_score for both the classifiers. Also print the classification report. Which of the 2 classifiers performs better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_pred: (490,)\n"
     ]
    }
   ],
   "source": [
    "#1 Decision Tree classifier\n",
    "model_dt = DTree(criterion='entropy')\n",
    "model_dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = model_dt.predict(X_test)\n",
    "print('Shape of y_pred:', y_pred_dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.38\n",
      "Recall:  0.40\n",
      "F1-score:  0.39\n",
      "[[  0   0   2   1   0   0   0]\n",
      " [  0   7   7   4   0   0   0]\n",
      " [  0   2  90  36  12   4   0]\n",
      " [  3   9  36 138  25   3   1]\n",
      " [  0   2   8  16  62   6   0]\n",
      " [  0   0   1   3   3   8   1]\n",
      " [  0   0   0   0   0   0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.35      0.39      0.37        18\n",
      "           5       0.62      0.62      0.62       144\n",
      "           6       0.70      0.64      0.67       215\n",
      "           7       0.61      0.66      0.63        94\n",
      "           8       0.38      0.50      0.43        16\n",
      "           9       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.62       490\n",
      "   macro avg       0.38      0.40      0.39       490\n",
      "weighted avg       0.63      0.62      0.63       490\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#2 DT Scores\n",
    "print(\"Precision: %0.2f\" %precision_score(y_test, y_pred_dt , average=\"macro\"))\n",
    "print(\"Recall:  %0.2f\" %recall_score(y_test, y_pred_dt , average=\"macro\"))\n",
    "print(\"F1-score:  %0.2f\" %f1_score(y_test, y_pred_dt , average=\"macro\"))\n",
    "print(confusion_matrix(y_test, y_pred_dt))\n",
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_pred: (490,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#1 Multinomial NB classifier\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_nb = model_nb.predict(X_test)\n",
    "print('Shape of y_pred:', y_pred_nb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.07\n",
      "Recall:  0.17\n",
      "F1-score:  0.10\n",
      "[[  0   0   0   3   0   0]\n",
      " [  0   0   0  18   0   0]\n",
      " [  0   0   0 144   0   0]\n",
      " [  0   0   0 215   0   0]\n",
      " [  0   0   0  94   0   0]\n",
      " [  0   0   0  16   0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         3\n",
      "           4       0.00      0.00      0.00        18\n",
      "           5       0.00      0.00      0.00       144\n",
      "           6       0.44      1.00      0.61       215\n",
      "           7       0.00      0.00      0.00        94\n",
      "           8       0.00      0.00      0.00        16\n",
      "\n",
      "    accuracy                           0.44       490\n",
      "   macro avg       0.07      0.17      0.10       490\n",
      "weighted avg       0.19      0.44      0.27       490\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\balav\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#2 Multinomial NB Scores\n",
    "print(\"Precision: %0.2f\" %precision_score(y_test, y_pred_nb , average=\"macro\"))\n",
    "print(\"Recall:  %0.2f\" %recall_score(y_test, y_pred_nb , average=\"macro\"))\n",
    "print(\"F1-score:  %0.2f\" %f1_score(y_test, y_pred_nb , average=\"macro\"))\n",
    "print(confusion_matrix(y_test, y_pred_nb))\n",
    "print(classification_report(y_test, y_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of the 2 models\n",
    "\n",
    "- The Precision, Recall & F1 scores for DecisionTree model is around 0.37, whereas the same for Multinomial NB model is between 0.7 - 0.17.\n",
    "- Based on this analysis, the DecisionTree model performs better for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Feature set (A,B,C) is good?  (10 points)\n",
    "Hint: Take Precision, Recall and F1-Score into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Feature set A performs better than B & C since the Precision, Recall & F1 scores are higher for feature set A compared to the other 2 feature sets.\n",
    "- For feature set A, Decision Tree model performs better compared to Multinomial NB.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (dsba6156)",
   "language": "python",
   "name": "pycharm-67ea9c55"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define functions to extract different attributes from html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_job_title(_tag: BeautifulSoup) -> str:\n",
    "    _a: BeautifulSoup = _tag.findAll(name='a', attrs={'data-tn-element': 'jobTitle'})\n",
    "    _title = _a[0].get('title') if len(_a) > 0 else ''\n",
    "    return _title\n",
    "\n",
    "def get_job_location(_tag: BeautifulSoup) -> str:\n",
    "    _div: BeautifulSoup = _tag.findAll(name='div', attrs={'class': 'recJobLoc'})\n",
    "    _location = _div[0].get('data-rc-loc') if len(_div) > 0 else ''\n",
    "    return _location\n",
    "\n",
    "def get_company_name(_tag: BeautifulSoup) -> str:\n",
    "    _a: BeautifulSoup = _tag.findAll(name='a', attrs={'data-tn-element': 'companyName'})\n",
    "    _company = _a[0].contents[0] if len(_a) > 0 else ''\n",
    "    return _company.replace('\\n', '')\n",
    "\n",
    "def get_job_salary(_tag: BeautifulSoup) -> str:\n",
    "    _div: BeautifulSoup = _tag.findAll(name='span', attrs={'class': 'salaryText'})\n",
    "    _salary = _div[0].contents[0] if len(_div) > 0 else ''\n",
    "    return _salary.replace('\\n', '')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Function to calculate minimum & maximum salaries from the text\n",
    "- Splits the salary range into minimum & maximum\n",
    "- Converts the salary to a number\n",
    "- Converts it to an annual salary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def calc_annual_salary(_salary: str) -> list:\n",
    "    if len(_salary) == 0:\n",
    "        return [0, 0]\n",
    "    _salaries = re.findall(r'\\$[0-9.,]+', _salary)\n",
    "    _salaries = [re.sub('[$,]', '', s) for s in _salaries]\n",
    "    if 'year' in _salary:\n",
    "        return [float(s) for s in _salaries]\n",
    "    if 'month' in _salary:\n",
    "        return [float(s) * 12 for s in _salaries]\n",
    "    if 'hour' in _salary:\n",
    "        return [float(s) * 2000 for s in _salaries]\n",
    "    return [0, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Functions to to build dynamic urls & invoke the urls & build job data by extracting html tags"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def extract_jobs_for_page(_soup: BeautifulSoup, _topic: str) -> list:\n",
    "    _jobs_for_page = []\n",
    "    for div in _soup.body.findAll(name='div', attrs={'class': 'unifiedRow'}):\n",
    "        _job = {\n",
    "            'topic': _topic,\n",
    "            'title': get_job_title(div), \n",
    "            'company': get_company_name(div),\n",
    "            'location': get_job_location(div),\n",
    "            'salary': get_job_salary(div),\n",
    "        }\n",
    "        _salaries = calc_annual_salary(_job['salary'])\n",
    "        _job['min_salary'] = _salaries[0]\n",
    "        _job['max_salary'] = _salaries[0]\n",
    "        if len(_salaries) > 1:\n",
    "            _job['max_salary'] = _salaries[1]\n",
    "        _jobs_for_page.append(_job)\n",
    "    return _jobs_for_page\n",
    "\n",
    "def build_url(_topic: str, _city: str, _start: int) -> str:\n",
    "    _topic = _topic.lower().replace(' ', '+')\n",
    "    return 'https://www.indeed.com/jobs?q=' + _topic + '&l=' + _city + '&start=' + str(_start)\n",
    "\n",
    "def invoke_url(_url: str) -> BeautifulSoup:\n",
    "    time.sleep(1)\n",
    "    _response = requests.get(url)\n",
    "    _soup = BeautifulSoup(_response.text, 'html.parser')\n",
    "    return _soup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Start processing the web scraping task\n",
    "- Fetch for the 3 search topics specified - Machine Learning, Data Scientist, Software Developer\n",
    "- Fetch for the 15 cities specified below\n",
    "- Fetch a maximum of 100 jobs (10 per page x 10 pages) for each city + search topic combination"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Processing - Machine Learning - Charlotte%2C+NC\n",
      "Processing - Machine Learning - New+York%2C+NY\n",
      "Processing - Machine Learning - San+Francisco%2C+CA\n",
      "Processing - Machine Learning - Atlanta%2C+GA\n",
      "Processing - Machine Learning - Dallas%2C+TX\n",
      "Processing - Machine Learning - Houston%2C+TX\n",
      "Processing - Machine Learning - Raleigh%2C+NC\n",
      "Processing - Machine Learning - Phoenix%2C+AZ\n",
      "Processing - Machine Learning - Los+Angeles%2C+CA\n",
      "Processing - Machine Learning - Seattle%2C+WA\n",
      "Processing - Machine Learning - Columbus%2C+OH\n",
      "Processing - Machine Learning - St.+Louis%2C+MO\n",
      "Processing - Machine Learning - Cedar+Rapids%2C+IA\n",
      "Processing - Machine Learning - Denver%2C+CO\n",
      "Processing - Machine Learning - Minneapolis%2C+MN\n",
      "Processing - Data Scientist - Charlotte%2C+NC\n",
      "Processing - Data Scientist - New+York%2C+NY\n",
      "Processing - Data Scientist - San+Francisco%2C+CA\n",
      "Processing - Data Scientist - Atlanta%2C+GA\n",
      "Processing - Data Scientist - Dallas%2C+TX\n",
      "Processing - Data Scientist - Houston%2C+TX\n",
      "Processing - Data Scientist - Raleigh%2C+NC\n",
      "Processing - Data Scientist - Phoenix%2C+AZ\n",
      "Processing - Data Scientist - Los+Angeles%2C+CA\n",
      "Processing - Data Scientist - Seattle%2C+WA\n",
      "Processing - Data Scientist - Columbus%2C+OH\n",
      "Processing - Data Scientist - St.+Louis%2C+MO\n",
      "Processing - Data Scientist - Cedar+Rapids%2C+IA\n",
      "Processing - Data Scientist - Denver%2C+CO\n",
      "Processing - Data Scientist - Minneapolis%2C+MN\n",
      "Processing - Software Developer - Charlotte%2C+NC\n",
      "Processing - Software Developer - New+York%2C+NY\n",
      "Processing - Software Developer - San+Francisco%2C+CA\n",
      "Processing - Software Developer - Atlanta%2C+GA\n",
      "Processing - Software Developer - Dallas%2C+TX\n",
      "Processing - Software Developer - Houston%2C+TX\n",
      "Processing - Software Developer - Raleigh%2C+NC\n",
      "Processing - Software Developer - Phoenix%2C+AZ\n",
      "Processing - Software Developer - Los+Angeles%2C+CA\n",
      "Processing - Software Developer - Seattle%2C+WA\n",
      "Processing - Software Developer - Columbus%2C+OH\n",
      "Processing - Software Developer - St.+Louis%2C+MO\n",
      "Processing - Software Developer - Cedar+Rapids%2C+IA\n",
      "Processing - Software Developer - Denver%2C+CO\n",
      "Processing - Software Developer - Minneapolis%2C+MN\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "max_jobs = 10\n",
    "topics = ['Machine Learning', 'Data Scientist', 'Software Developer']\n",
    "cities = ['Charlotte%2C+NC', 'New+York%2C+NY', 'San+Francisco%2C+CA', 'Atlanta%2C+GA', 'Dallas%2C+TX',\n",
    "          'Houston%2C+TX', 'Raleigh%2C+NC', 'Phoenix%2C+AZ', 'Los+Angeles%2C+CA', 'Seattle%2C+WA', \n",
    "          'Columbus%2C+OH', 'St.+Louis%2C+MO', 'Cedar+Rapids%2C+IA', 'Denver%2C+CO', 'Minneapolis%2C+MN']\n",
    "\n",
    "all_jobs = []\n",
    "for topic in topics:\n",
    "    for city in cities:\n",
    "        print('Processing -', topic, '-', city)\n",
    "        for start in range(0, max_jobs, 10):\n",
    "            url = build_url(topic, city, start)\n",
    "            soup = invoke_url(url)\n",
    "            jobs_for_page = extract_jobs_for_page(soup, topic)\n",
    "            all_jobs.extend(jobs_for_page)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Build a DataFrame & store as CSV"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(779, 7)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=all_jobs)\n",
    "print(df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "File write completed.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "df.to_csv('./out_data/all_jobs.csv')\n",
    "print('File write completed.')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-67ea9c55",
   "language": "python",
   "display_name": "PyCharm (dsba6156)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}